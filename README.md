# ðŸš— Simple-BEV ONNX Export Pipeline

> **Week 2 Assignment** â€” Optimize the Simple-BEV model, export to ONNX, and verify ONNX Runtime  
> Reference: [aharley/simple_bev](https://github.com/aharley/simple_bev)

---

## ðŸ“Œ Overview

This project implements a **Bird's Eye View (BEV) perception model** for autonomous driving, exports it to ONNX format, optimizes it with ONNX Simplifier, and benchmarks it against PyTorch baseline.

| Item | Detail |
|------|--------|
| Model | SimpleBEV (custom implementation) |
| Input | `[B, 6, 3, 224, 400]` â€” 6 camera views |
| Output | `[B, 8, 200, 200]` â€” BEV segmentation map |
| Parameters | ~7.8M |
| ONNX Opset | 17 |
| Runtime | ONNX Runtime 1.24.1 |
| GPU (export) | Tesla T4 (Google Colab) |

---

## ðŸ“ Directory Structure

```
simple-bev-onnx/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/simple_bev.py        # Model architecture
â”‚   â”œâ”€â”€ training/train.py           # Training script
â”‚   â”œâ”€â”€ inference/inference.py      # ONNX Runtime inference
â”‚   â””â”€â”€ data/datacard.md            # Data card
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.sh             # â­ Master: runs everything
â”‚   â”œâ”€â”€ setup.sh                    # Install dependencies
â”‚   â”œâ”€â”€ train.sh                    # Train model
â”‚   â”œâ”€â”€ export.sh                   # Export to ONNX
â”‚   â”œâ”€â”€ infer.sh                    # Run inference
â”‚   â””â”€â”€ benchmark.sh                # PyTorch vs ONNX comparison
â”œâ”€â”€ configs/config.yaml             # Hyperparameters & paths
â”œâ”€â”€ artifacts/benchmark_results.txt # Benchmark report
â”œâ”€â”€ docs/deployment_guide.md        # Deployment guide
â”œâ”€â”€ notebooks/                      # Colab notebook
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸš€ Quick Start

### â­ Option 1: Full Pipeline (One Command)
```bash
git clone https://github.com/ARNiteshKumar/MAGIC-Cluster_ML-Models_1
cd simple-bev-onnx
bash scripts/run_pipeline.sh
```
This runs: **Setup â†’ Train â†’ Export ONNX â†’ Inference â†’ Benchmark** automatically.

### Option 2: Step by Step
```bash
bash scripts/setup.sh       # 1. Install dependencies
bash scripts/train.sh       # 2. Train model
bash scripts/export.sh      # 3. Export to ONNX
bash scripts/infer.sh       # 4. Run inference
bash scripts/benchmark.sh   # 5. Benchmark report
```

### Skip Training (use pre-trained weights from Release)
```bash
bash scripts/run_pipeline.sh --skip-train
```

### Run in Docker
```bash
docker build -t simple-bev-onnx .
docker run --rm simple-bev-onnx
```

---

## ðŸ“Š Benchmark Results

Tested on **Google Colab T4 GPU** (ONNX Runtime on CPU for portability):

| Metric | PyTorch CPU | ONNX Runtime | Speedup |
|--------|------------|--------------|---------|
| Mean Latency | 504.02 ms | 393.70 ms | **1.28x** |
| P95 Latency | 664.65 ms | 563.65 ms | 1.18x |
| Throughput | 1.98 FPS | 2.54 FPS | â€” |
| Output Shape | [1,8,200,200] | [1,8,200,200] | âœ… Match |
| Numerical Diff | â€” | 1.49e-08 | âœ… Valid |

> ðŸš€ ONNX Runtime is **1.28x faster** than PyTorch on CPU  
> âœ… Numerical verification PASSED (max diff: 1.49e-08)

---

## ðŸ§  Model Architecture

```
SimpleBEVModel
â”œâ”€â”€ BEVEncoder     (ResNet-style backbone)
â”œâ”€â”€ BEVSplat       (Feature projection to BEV grid)
â”œâ”€â”€ FusionLayer    (Fuses all 6 camera features)
â””â”€â”€ BEVDecoder     (Upsample + segmentation head)
```

---

## ðŸ“¦ Model Artifacts

> âš ï¸ ONNX model files are tracked via [Git Releases](../../releases).  
> Re-generate locally by running the notebook.

| File | Description |
|------|-------------|
| `simple_bev.onnx` | Base ONNX export (opset 17) |
| `simple_bev_optimized.onnx` | onnxsim-optimized ONNX |
| `simple_bev.pt` | PyTorch state dict |

---

## ðŸ”— References

- [Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?](https://github.com/aharley/simple_bev)
- [ONNX Runtime Docs](https://onnxruntime.ai/docs/)
- [ONNX Simplifier](https://github.com/daquexian/onnx-simplifier)
